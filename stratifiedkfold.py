# -*- coding: utf-8 -*-
"""stratifiedkfold.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zwTAtsG7P_ATr-CtgCVue9G9GvEmPHs0

### Imports
"""

import json
import numpy as np
import os.path
import pandas as pd
# import seaborn as sns
import matplotlib.pyplot as plt

from keras import layers, Input, Model
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

# %matplotlib inline

"""### Loading data"""

#from google.colab import drive
#drive.mount('/content/drive')

dataset_fp = 'osg2018_correct_formatted_normalized_400-1000.csv'

print("Local copy of the dataset file: {}".format(dataset_fp))
# !head -n5 {'/content/drive/My\ Drive/Ag_2018_Final_products/Paul\ code/osg2018_correct_formatted_normalized_400-1000.csv'}

print('Reading data from CSV...')
all_raw = pd.read_csv(dataset_fp)
print('Data read successfully')

all_raw = all_raw.sample(frac=1)

train_raw, test_raw = train_test_split(all_raw,
                                        test_size=0.10,
                                        random_state=42, stratify=all_raw.Status)

print(train_raw.head())

print(test_raw.head())

print(train_raw.shape, test_raw.shape)

print(train_raw.isnull().sum().sum(), test_raw.isnull().sum().sum())

"""So there are no missing values in either training or test set.

### Variety distribution
"""
'''
sns.countplot(train_raw.Variety)
plt.show()
sns.countplot(test_raw.Variety)
plt.show()
'''
"""### Target distribution"""
'''
sns.countplot(train_raw.Status)
plt.show()
'''
print(train_raw.Status.value_counts())

"""Looks like class labels are uniformly distributed in training data.

### Categorical Feature
"""

trn_variety = pd.get_dummies(train_raw['Variety'])
test_variety = pd.get_dummies(test_raw['Variety'])

print(trn_variety.shape, test_variety.shape)

"""### Normalize features"""

target = train_raw.Status
test_target = test_raw.Status

train_raw.drop(['Variety', 'Status'], axis=1, inplace=True)
test_raw.drop(['Variety', 'Status'], axis=1, inplace=True)

sc = StandardScaler()
train_normalized = sc.fit_transform(train_raw)
test_normalized = sc.transform(test_raw)

train_normalized = np.concatenate([train_normalized, trn_variety.values], axis=1)
test_normalized = np.concatenate([test_normalized, test_variety.values], axis=1)

col_names = np.concatenate([train_raw.columns, trn_variety.columns], axis=0)
print(col_names)

"""### Model"""

def build_model(num_features):
    inp = Input(shape=(num_features,), name='input')
    x = layers.Dense(num_features//2+1, activation='relu')(inp)
    x = layers.Dropout(0.6)(x)
    x = layers.Dense(1, activation='sigmoid')(x)
    
    model = Model(inp, x)
    model.compile(optimizer='adam',
                 loss='binary_crossentropy', metrics=['acc'])
    
    return model

model = build_model(train_normalized.shape[1])
model.summary()

"""### Training"""

from keras import backend as K
K.tf.compat.v1.logging.set_verbosity(K.tf.compat.v1.logging.ERROR)

NFOLDS = 5
NEPOCHS = 20000
folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)

def plot_history(history):
  # list all data in history
  # print(history.history.keys())
  # summarize history for accuracy
  plt.plot(history.history['acc'])
  plt.plot(history.history['val_acc'])
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()
  # summarize history for loss
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

UPPER_SPECTRAL_LIMIT = 1000
LOWER_SPECTRAL_LIMIT = 400
NUM_BANDS = 4 #len(col_names)-3
BAND_WIDTH = 10
BAND_SEPARATION = 10

def customize_subset(features, normalized_data):
    data_x = normalized_data[:,0:1]
    for feature_idx in features:
        feature = int(col_names[feature_idx])
        features_idcs_to_avg = []
        for other_idx in range(max(1,feature_idx-BAND_WIDTH//2), min(len(col_names)-2,feature_idx+BAND_WIDTH//2+1)):
          other_f = int(col_names[other_idx])
          if (abs(feature - other_f) <= BAND_WIDTH/2):
              features_idcs_to_avg.append(other_idx)
        data_x = np.concatenate([data_x,np.mean(normalized_data[:,features_idcs_to_avg],axis=1)[:,None]], axis=1)
    data_x = np.concatenate([data_x,normalized_data[:,-2:]], axis=1)
    return data_x

def get_test_accuracy(genome):
  train_x = customize_subset(genome, train_normalized)
  test_x = customize_subset(genome, test_normalized)
  # train_x = train_normalized[:,[0]+genome+[len(col_names)-2,len(col_names)-1]]
  # test_x = test_normalized[:,[0]+genome+[len(col_names)-2,len(col_names)-1]]
  
  avg_test_acc_scores = 0.0
  trn_acc = 0

  oof = np.zeros(train_raw.shape[0])
  predictions = np.zeros(test_raw.shape[0])

  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_x, target)):
      print(f'.', end='')

      weights_path = f'weights.best.hdf5.{NUM_BANDS}_{BAND_WIDTH}_{BAND_SEPARATION}/{str(genome).strip("[]")}.fold_{fold_}'
      monitor_mode = 'val_loss'
      # TODO: Random Restarts with different weights, keep the one with the best validation accuracy
      val_acc_checkpoint = ModelCheckpoint(weights_path, monitor=monitor_mode, verbose=0, save_best_only=True)
      reduceLR = ReduceLROnPlateau(monitor=monitor_mode, patience=500, verbose=0, min_lr=1e-6)
      es = EarlyStopping(monitor=monitor_mode, min_delta=0, patience=1501, verbose=0)

      model = build_model(train_x.shape[1])
      history = model.fit(train_x[trn_idx], target.values[trn_idx], epochs=NEPOCHS, validation_data=(train_x[val_idx], target.values[val_idx]),
           callbacks=[val_acc_checkpoint, reduceLR, es], verbose=0, shuffle=True)
      #plot_history(history)
      
      model.load_weights(weights_path)

      trn_preds = model.predict(train_x[trn_idx], batch_size=2048, verbose=0)
      trn_acc_score = accuracy_score(target.values[trn_idx], trn_preds.round())
      trn_acc += trn_acc_score
      #print(f'\tTRAINING ACCURACY: {trn_acc_score}')

      val_preds = model.predict(train_x[val_idx], batch_size=2048, verbose=0)
      #print(f'\tVALIDATION ACCURACY: {accuracy_score(target.values[val_idx], val_preds.round())}')

      test_preds = model.predict(test_x, batch_size=2048, verbose=0)
      test_acc_score = accuracy_score(test_target.values, test_preds.round())
      #print(f'\tTEST ACCURACY: {test_acc_score}\n')
      avg_test_acc_scores += test_acc_score/folds.n_splits

      #print(f'\tTRAINING ROC AUC: {roc_auc_score(target.values[trn_idx], trn_preds.reshape(-1))}')
      #print(f'\tVALIDATION ROC AUC: {roc_auc_score(target.values[val_idx], val_preds.reshape(-1))}')
      #print(f'\tTEST ROC AUC: {roc_auc_score(test_target.values, test_preds.reshape(-1))}')

      oof[val_idx] = val_preds.reshape(-1)
      predictions += test_preds.reshape(-1)/folds.n_splits

  # print(f'AVG TEST ACCURACY: {avg_test_acc_scores}')
  test_acc_scores_of_combined_models = accuracy_score(test_target.values, predictions.round())
  print(f'TRAINING ACCURACY: {trn_acc/folds.n_splits}')
  print(f'VALIDATION ACCURACY: {accuracy_score(target.values, oof.round())}')
  print(f'TEST ACCURACY OF COMBINED MODELS: {test_acc_scores_of_combined_models}')

  # print(f'OOF ROC AUC: {roc_auc_score(target.values, oof)}')
  
  return test_acc_scores_of_combined_models

"""### Genetic Algorithm"""

import random

POP_SIZE = 10
OFFSPRING_SIZE = 10
GENERATIONS = 1000

TOTAL_FEATURES = train_normalized.shape[1]
print(f'Total Feature Count: {TOTAL_FEATURES}')

def add_full_band(features):
  full_band = []
  for feature_idx in features:
    feature = int(col_names[feature_idx])
    for other_idx in range(max(1,feature_idx-BAND_WIDTH//2), min(len(col_names)-2,feature_idx+BAND_WIDTH//2+1)):
      other_f = int(col_names[other_idx])
      if (abs(feature - other_f) <= BAND_WIDTH/2):
        full_band.append(other_idx)
  
  #print("Full band:",col_names[full_band])
  return full_band

genome_scores = {}
def fitness(genome):
  print(f'Fitness (accuracy) for {col_names[genome]} = ', end='')
  genome_id = str(genome).strip('[]')
  if genome_id in genome_scores:
    print(f'{genome_scores[genome_id]} (prev calc)')
    return genome_scores[genome_id]
  
  genome_scores[genome_id] = get_test_accuracy(genome)
  
  print(f'{genome_scores[genome_id]}')

  return genome_scores[genome_id]

def sample_features(features_so_far):
  while len(features_so_far) < NUM_BANDS:
    next_feature_idx = random.randint(1, len(col_names)-3)
    next_feature = int(col_names[next_feature_idx])
    #print("Considering addition of", next_feature,"to",col_names[features_so_far])

    if (next_feature - LOWER_SPECTRAL_LIMIT) < BAND_WIDTH/2 \
        or (UPPER_SPECTRAL_LIMIT - next_feature) < BAND_WIDTH/2:
        #print("\tToo close to edge of spectral range")
        continue
    
    add_feature = True
    for feature_idx in features_so_far:
      feature = int(col_names[feature_idx])
      
      if abs(feature - next_feature) < (BAND_WIDTH + BAND_SEPARATION):
        #print("\tConflict with", feature,"FAILING...")
        add_feature = False
        break
        
    if add_feature:
      #print("\tNo conflicts, ADDING...")
      features_so_far.append(next_feature_idx)
    
  return features_so_far

RESUME_TRAINING = True
SAVE_FILE = "population.json"
GENOME_SCORES_FILE = "genome_scores.json"
        
def init_population():
  if RESUME_TRAINING and os.path.exists(SAVE_FILE):
      print("Resuming training...")
      with open(SAVE_FILE,"r") as f:
        population = json.load(f)
      f.close()
  else:
      population = []
      for i in range(POP_SIZE):
        genome = sorted(sample_features([]))
        population.append((genome,fitness(genome)))

  if RESUME_TRAINING and os.path.exists(GENOME_SCORES_FILE):
      print("Loading genome scores...")
      with open(GENOME_SCORES_FILE) as g:
          genome_scores = json.load(g)
      g.close()
    
  return population

def cross_over(genome1, genome2):
  cross_idx = random.randint(1,len(genome1)-1)
  #print("Crossover point:",cross_idx)
  return genome1[:cross_idx] + genome2[cross_idx:]

MUTATION_RATE = 0.3
def mutate(genome):
  for i in range(len(genome)):
    if random.uniform(0,1) < MUTATION_RATE:
      #print("Mutating position:",i,col_names[genome[i]])
      del genome[i]
      sample_features(genome)
      genome.insert(i, genome.pop())
  genome = sorted(genome)
  return (genome, fitness(genome))

print("Initializing population...")
population = init_population()

print(population)

def save_population(population):
    with open(GENOME_SCORES_FILE,"w") as g:
        json.dump(genome_scores,g)
    g.close()
    with open(SAVE_FILE,"w") as f:
        json.dump(population,f)
    f.close()

for gen in range(GENERATIONS):
  next_generation = []
  save_population(population)

  print(f'Generation {gen}:')
  while len(next_generation) < OFFSPRING_SIZE:
    parent1 = random.choice(population)
    parent2 = random.choice(population)
    while parent1 == parent2:
          parent2 = random.choice(population)
    #print("Crossing", parent1,"and",parent2)
    baby = mutate(cross_over(parent1[0], parent2[0]))
    #print("Baby:",baby)
    if baby not in next_generation and baby not in population:
    	next_generation.append(baby)
    
  population.extend(next_generation)
  dedupedPopulation = []
  for g in population:
    if g not in dedupedPopulation:
      dedupedPopulation.append(g)
  population = dedupedPopulation

  population = sorted(population, key=lambda x: x[1], reverse=True)

  population = population[:POP_SIZE]
  
  print(col_names[population[0][0]],population[0][1])
